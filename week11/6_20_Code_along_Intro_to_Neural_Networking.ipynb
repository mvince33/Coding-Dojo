{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "6.20 Code-along Intro to Neural Networking.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mvince33/Coding-Dojo/blob/main/week11/6_20_Code_along_Intro_to_Neural_Networking.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGKAZlCh02cl"
      },
      "source": [
        "# Introduction to Neural Networking in Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OStduN-207_9"
      },
      "source": [
        "We will use the version of Keras that comes in the Tensorflow package, as it has the most up to date tools.\n",
        "\n",
        "Keras works as weapper for deep learning model to be used as classification or regression estimators in sklearn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zw6CH1mp0zR4"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from seaborn import heatmap\n",
        "\n",
        "from sklearn.metrics import mean_squared_error, classification_report, \\\n",
        "ConfusionMatrixDisplay\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# new libraries\n",
        "import tensorflow.keras as keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can find source of the below visualization function here :[source](https://towardsdatascience.com/deep-learning-with-python-neural-networks-complete-tutorial-6b53c0b06af0)\n"
      ],
      "metadata": {
        "id": "zQkGeK7-vZ0b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Functions to Extract info for each layer in a keras model.\n",
        "'''\n",
        "def utils_nn_config(model):\n",
        "    lst_layers = []\n",
        "    if \"Sequential\" in str(model): #-> Sequential doesn't show the input layer\n",
        "        layer = model.layers[0]\n",
        "        lst_layers.append({\"name\":\"input\", \"in\":int(layer.input.shape[-1]), \"neurons\":0, \n",
        "                           \"out\":int(layer.input.shape[-1]), \"activation\":None,\n",
        "                           \"params\":0, \"bias\":0})\n",
        "    for layer in model.layers:\n",
        "        try:\n",
        "            dic_layer = {\"name\":layer.name, \"in\":int(layer.input.shape[-1]), \"neurons\":layer.units, \n",
        "                         \"out\":int(layer.output.shape[-1]), \"activation\":layer.get_config()[\"activation\"],\n",
        "                         \"params\":layer.get_weights()[0], \"bias\":layer.get_weights()[1]}\n",
        "        except:\n",
        "            dic_layer = {\"name\":layer.name, \"in\":int(layer.input.shape[-1]), \"neurons\":0, \n",
        "                         \"out\":int(layer.output.shape[-1]), \"activation\":None,\n",
        "                         \"params\":0, \"bias\":0}\n",
        "        lst_layers.append(dic_layer)\n",
        "    return lst_layers\n",
        "\n",
        "def visualize_nn(model, description=False, figsize=(10,8)):\n",
        "    ## get layers info\n",
        "    lst_layers = utils_nn_config(model)\n",
        "    layer_sizes = [layer[\"out\"] for layer in lst_layers]\n",
        "    \n",
        "    ## fig setup\n",
        "    fig = plt.figure(figsize=figsize)\n",
        "    ax = fig.gca()\n",
        "    ax.set(title=model.name)\n",
        "    ax.axis('off')\n",
        "    left, right, bottom, top = 0.1, 0.9, 0.1, 0.9\n",
        "    x_space = (right-left) / float(len(layer_sizes)-1)\n",
        "    y_space = (top-bottom) / float(max(layer_sizes))\n",
        "    p = 0.025\n",
        "    \n",
        "    ## nodes\n",
        "    for i,n in enumerate(layer_sizes):\n",
        "        top_on_layer = y_space*(n-1)/2.0 + (top+bottom)/2.0\n",
        "        layer = lst_layers[i]\n",
        "        color = \"green\" if i in [0, len(layer_sizes)-1] else \"blue\"\n",
        "        color = \"red\" if (layer['neurons'] == 0) and (i > 0) else color\n",
        "        \n",
        "        ### add description\n",
        "        if (description is True):\n",
        "            d = i if i == 0 else i-0.5\n",
        "            if layer['activation'] is None:\n",
        "                plt.text(x=left+d*x_space, y=top, fontsize=10, color=color, s=layer[\"name\"].upper())\n",
        "            else:\n",
        "                plt.text(x=left+d*x_space, y=top, fontsize=10, color=color, s=layer[\"name\"].upper())\n",
        "                plt.text(x=left+d*x_space, y=top-p, fontsize=10, color=color, s=layer['activation']+\" (\")\n",
        "                plt.text(x=left+d*x_space, y=top-2*p, fontsize=10, color=color, s=\"Î£\"+str(layer['in'])+\"[X*w]+b\")\n",
        "                out = \" Y\"  if i == len(layer_sizes)-1 else \" out\"\n",
        "                plt.text(x=left+d*x_space, y=top-3*p, fontsize=10, color=color, s=\") = \"+str(layer['neurons'])+out)\n",
        "        \n",
        "        ### circles\n",
        "        for m in range(n):\n",
        "            color = \"limegreen\" if color == \"green\" else color\n",
        "            circle = plt.Circle(xy=(left+i*x_space, top_on_layer-m*y_space-4*p), radius=y_space/4.0, color=color, ec='k', zorder=4)\n",
        "            ax.add_artist(circle)\n",
        "            \n",
        "            ### add text\n",
        "            if i == 0:\n",
        "                plt.text(x=left-4*p, y=top_on_layer-m*y_space-4*p, fontsize=10, s=r'$X_{'+str(m+1)+'}$')\n",
        "            elif i == len(layer_sizes)-1:\n",
        "                plt.text(x=right+4*p, y=top_on_layer-m*y_space-4*p, fontsize=10, s=r'$y_{'+str(m+1)+'}$')\n",
        "            else:\n",
        "                plt.text(x=left+i*x_space+p, y=top_on_layer-m*y_space+(y_space/8.+0.01*y_space)-4*p, fontsize=10, s=r'$H_{'+str(m+1)+'}$')\n",
        "    \n",
        "    ## links\n",
        "    for i, (n_a, n_b) in enumerate(zip(layer_sizes[:-1], layer_sizes[1:])):\n",
        "        layer = lst_layers[i+1]\n",
        "        color = \"green\" if i == len(layer_sizes)-2 else \"blue\"\n",
        "        color = \"red\" if layer['neurons'] == 0 else color\n",
        "        layer_top_a = y_space*(n_a-1)/2. + (top+bottom)/2. -4*p\n",
        "        layer_top_b = y_space*(n_b-1)/2. + (top+bottom)/2. -4*p\n",
        "        for m in range(n_a):\n",
        "            for o in range(n_b):\n",
        "                line = plt.Line2D([i*x_space+left, (i+1)*x_space+left], \n",
        "                                  [layer_top_a-m*y_space, layer_top_b-o*y_space], \n",
        "                                  c=color, alpha=0.5)\n",
        "                if layer['activation'] is None:\n",
        "                    if o == m:\n",
        "                        ax.add_artist(line)\n",
        "                else:\n",
        "                    ax.add_artist(line)\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "VFuKhUDnvbKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAUUTWm93ojP"
      },
      "source": [
        "# Data\n",
        "\n",
        "We will be working with 2 different datasets in this project, 1 is a regression dataset and the other is a classification dataset.  This way you can practice doing both using deep learning.\n",
        "\n",
        "**NOTE**\n",
        "\n",
        "These datasets are very small for deep learning.  Deep learning models usually work best with very large datasets with at least 10,000 or more samples.  They work best on even larger datasets than that.  But, for demonstration we will use these smaller datasets.\n",
        "\n",
        "## Regression\n",
        "This is a dataset of housing prices in Boston from 1978.  Each row is a house and the dataset includes several features regarding each house.  Our target today will be the price of the home.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_PM7Bt81FKh"
      },
      "source": [
        "regression_df = pd.read_csv('https://raw.githubusercontent.com/ninja-josh/image-storage/main/Boston_Housing_from_Sklearn.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YPAPvdG6Sqy"
      },
      "source": [
        "# Regression\n",
        "\n",
        "Let's start with modeling the regression dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3STtBugN3fp1"
      },
      "source": [
        "regression_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "regression_df.info()"
      ],
      "metadata": {
        "id": "4sJoxV1ZJKBd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvcvdJUZ6YKX"
      },
      "source": [
        "regression_df.duplicated().any()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IrxmHTaU6s3N"
      },
      "source": [
        "regression_df.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rU0zM_dk6xDd"
      },
      "source": [
        "# Define X and Y and complete the train test split\n",
        "X = regression_df.drop(columns = 'PRICE')\n",
        "y = regression_df['PRICE']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, random_state = 42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqS765vN8Xti"
      },
      "source": [
        "## Scaling\n",
        "\n",
        "Always scale your data for deep learning.  Otherwise you get a problem call 'Exploding Weights'.  Some weights will be updated much faster than others because the inputs are at larger scales.  This tends to hurt learning as data on smaller scales does not update as fast and doesn't get to contribute as much to the decision making process.  By scaling we put all features on the same footing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvCCtAhQ7gGe"
      },
      "source": [
        "# Scale the data\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5pga5rHChPu"
      },
      "source": [
        "### Plot History\n",
        "\n",
        "Since we will be plotting histories for all of our models, lets create a function to do it quickly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UbrKqMv0_28Q"
      },
      "source": [
        "# You can use this function to see how your model improves over time\n",
        "def plot_history(history, metric=None):\n",
        "  \"\"\"plot loss and passed metric.  metric is passed as string and must match 'metric'\n",
        "  argument in the compile step\"\"\"\n",
        "  fig, axes = plt.subplots(2,1, figsize = (5,10))\n",
        "  axes[0].plot(history.history['loss'], label = \"train\")\n",
        "  axes[0].plot(history.history['val_loss'], label='test')\n",
        "  axes[0].set_title('Loss')\n",
        "  axes[0].legend()\n",
        "  if metric:\n",
        "    axes[1].plot(history.history[metric], label = 'train')\n",
        "    axes[1].plot(history.history['val_' + metric], label = 'test')\n",
        "    axes[1].set_title(metric)\n",
        "    axes[1].legend()\n",
        "\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BzX4RBcu8TFx"
      },
      "source": [
        "## First Simple Model\n",
        "\n",
        "We always want to start simple, as deep learning models can get very complex fast and more complex models take more time to train and are more prone to overfitting.  A well performing simple model is better than a well performing complex model.\n",
        "\n",
        "## Input layer\n",
        "The first layer we will define is not technically the input layer.  We will define the first hidden layer with a special argument that tells Keras how to create a input layer:\n",
        "\n",
        "`input_dim=`\n",
        "\n",
        "Input layers can also be defined manually using tensorflow.keras.layers.InputLayer\n",
        "\n",
        "## Activation function\n",
        "\n",
        "For the single hidden layer we will try just 3 nodes and use a ReLU activation.  ReLUs tend to perform well for hidden layers.\n",
        "\n",
        "## Output Layer\n",
        "\n",
        "For out output layer (last layer) we just use one node because we only want the output of the model to be one number.  We will use a linear activation function.  This will simply output the value from the weights and bias in the node with no change.  The output will be a continuous number, a float.  This will make our model a regression model.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the shape of input\n",
        "X_train.shape[1]\n",
        "input_dim = X_train.shape[1]"
      ],
      "metadata": {
        "id": "DfYhlMbBwZcQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Note:\n",
        "### The first layer you define will NOT be the input layer!  Keras will create an input layer on its own, implicitly."
      ],
      "metadata": {
        "id": "glnm6hhh4M6u"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AnjwY1dz7vcG"
      },
      "source": [
        "# Instantiate your sequential model\n",
        "\n",
        "# Add first hidden layer with 3 neurons THIS IS NOT THE INPUT LAYER!\n",
        "# Tell Keras how to construct the input layer shape using input_dim\n",
        "\n",
        "# Add output layer with 1 node\n",
        "\n",
        "# Check summary of network \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize Network\n",
        "\n"
      ],
      "metadata": {
        "id": "52-IbmTuv8ED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wAFuCDGOBzo"
      },
      "source": [
        "## Compiling\n",
        "\n",
        "Compiling the model puts all the pieces together to make it ready to train.  \n",
        "\n",
        "For this step, we need to specify a few other hyperparameters:\n",
        "\n",
        "* **Optimizer:** An Adam optimizer is a favorite and often performs well, it's a good place to start.\n",
        "  - Other optimizers : Gradient Descent, Stochastic Gradient Descent, Adagrad, RMSProp\n",
        "* **Loss Function:** 'mse' or mean squared error.  This is the number our model will try to reduce in each epoch.  Since this is a regression model we want our model to minimize the mean squared error.  A loss function ALWAYS needs to be a measurement of the total error that the model can REDUCE.  R^2 won't work because higher is better. We don't want the model to reduce R^2!\n",
        "* **Metrics:** 'mae' or mean absolute error.  We can provide a list of any appropriate metrics we want the model to keep track at each epoch.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwAGvWCZN1WC"
      },
      "source": [
        "# Compile your model.\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBvS_wkr_hEh"
      },
      "source": [
        "# Training (AKA fitting)\n",
        "\n",
        "Let's try training our model for 100 few epochs.  Sometimes that is enough, and it will give us an idea whether our model is learning anything."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81VFx1n1-TS7"
      },
      "source": [
        "# Fit your model\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8Kox9hOOq7w"
      },
      "source": [
        "# Apply the custom function plot_history() to see how your model is doing\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Ic8eFsL_x2m"
      },
      "source": [
        "## Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33J5dOt3A8ZO"
      },
      "source": [
        "# Make predictions and evaluate your model\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWaTq6c7FHlr"
      },
      "source": [
        "\n",
        "# Classification:\n",
        "\n",
        "Classification models are similar, except that we need to adjust:\n",
        "* The final activation of the output layer, and\n",
        "* the loss function and metrics in the compile step.\n",
        "\n",
        "We will also need to do some processing of the predictions after training to make them integers instead of floats.\n",
        "\n",
        "### Remember: \n",
        "MAE, MSE, RMSE, and R2 are regression metrics,\n",
        "\n",
        "accuracy, recall, precision, and F1-Score are classification metrics.\n",
        "\n",
        "## Classification Dataset\n",
        "The classification dataset describes diabetes rates among Pima Indians.  Each row is a person and this dataset includes features regarding health related measurements.  The target is binary and represents whether or not a person will diagnosed with diabetes.  This is another old dataset first presented in 1988.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CN1zHoGcGeFN"
      },
      "source": [
        "classification_df = pd.read_csv('https://raw.githubusercontent.com/ninja-josh/image-storage/main/diabetes.csv')\n",
        "classification_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isTaHNFpGjQH"
      },
      "source": [
        "classification_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtZv-V73Gmjv"
      },
      "source": [
        "classification_df.duplicated().any()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdF3qT-9Gp_s"
      },
      "source": [
        "classification_df.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8cjiy3BGvLa"
      },
      "source": [
        "We see minimums for Glucose, BloodPression, SkinThickness, Insulin, and BMI of 0s.  Those are impossible for humans, so lets drop those rows."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TwzoeFVWG6Dd"
      },
      "source": [
        "no_glucose = classification_df['Glucose'] == 0\n",
        "no_blood = classification_df['BloodPressure'] == 0\n",
        "no_skin = classification_df['SkinThickness'] == 0\n",
        "no_insulin = classification_df['Insulin'] == 0\n",
        "no_bmi = classification_df['BMI'] == 0\n",
        "\n",
        "#class_df_clean excludes rows that have no values == 0 in the above columns\n",
        "class_df_clean = classification_df[~(no_glucose |\n",
        "                                     no_blood |\n",
        "                                     no_skin |\n",
        "                                     no_insulin |\n",
        "                                     no_bmi)]\n",
        "class_df_clean.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQqWsTbaM5It"
      },
      "source": [
        "We lost a lot of data, going from 768 samples to 392 samples.  In the future we might impute this data using means, medians, or other imputation strategies.  For this exercise we won't focus on that."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JjREwrWgGPtP"
      },
      "source": [
        "# Define X and y and train test split\n",
        "X = class_df_clean.drop(columns = 'Outcome')\n",
        "y = class_df_clean['Outcome']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42, stratify = y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKPiCOByPOfF"
      },
      "source": [
        "# Scale the data\n",
        "scaler = MinMaxScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TWU179nQIdt"
      },
      "source": [
        "## Build the Classification Model\n",
        "\n",
        "We need to do a few things differently here because this is a binary classification:\n",
        "\n",
        "1. The activation of our final layer needs to be 'sigmoid'.  \n",
        "\n",
        "\n",
        "(If this were multiclass classification, we would set the final activation as 'softmax' and the number of output nodes would be the number of classes in our y_train.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAwy3AXrPjwH"
      },
      "source": [
        "# Build your model\n",
        "\n",
        "\n",
        "# One output node with 'sigmoid' activation\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize network\n"
      ],
      "metadata": {
        "id": "rw1mjS73294G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nrwLXyKPpO1"
      },
      "source": [
        "## More Changes for Classification:\n",
        "\n",
        "1.  We need to change our loss to 'binary_crossentropy'.  If this were multiclass we would use 'categorical_crossentrobpy'.\n",
        "\n",
        "2. Our metrics should be classification metrics.  We will use accuracy.  We could also use recall or precision. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOe9CMP2PtDT"
      },
      "source": [
        "# # Compile your model, set metrics = ['acc']\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vScTvfoKOVLO"
      },
      "source": [
        "# fit your model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKLxgYDcP0_H"
      },
      "source": [
        "# See how your model is doing\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LD18RKjYTFXP"
      },
      "source": [
        "## Evaluation\n",
        "\n",
        "Keras models always output floats, not integers.  In this case the final sigmoid activation function will return a number between 0 and 1.  If the number is closer to 1, the model predicts the sample is more likely to be class 1.  If it is closer to 2, the sample is predicted to be more likely to be class 0.  \n",
        "\n",
        "This is similar to the output of .predict_proba() with Scikit-Learn models.\n",
        "\n",
        "### Converting Floats to Ints\n",
        "\n",
        "In order to use Scikit-Learn metrics functions, the float outputs of the model need to be converted to ints.  We don't want to just use `int(pred)` or `pred.astype(int)` because that will just drop the decimal and all our predictions would be 0s.  \n",
        "\n",
        "Instead we want to **round** the predictions to the nearest integer. To round all of the numbers in an array we can use the NumPy function, `np.rint()` which is short for 'round to integer'.  "
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u1CSEpv4YXV7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sov8ZJUHP-Bf"
      },
      "source": [
        "\n",
        "# Define labels for the confusion matrix\n",
        "\n",
        "# Get training predictions and round them to integers instead of floats\n",
        "\n",
        "# Classification Report\n",
        "\n",
        "# Confusion Matrix\n",
        "\n",
        "# Get testing predictions and round them to integers\n",
        "# Confusion Matrix\n",
        "\n",
        "# Classification report\n",
        "\n",
        "# Confusion Matrix\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sy5rikbBZUQK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}